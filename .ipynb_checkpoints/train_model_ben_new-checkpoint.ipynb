{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 19:32:27.510729: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac065e8-fcf2-44a0-8dee-e1e44e605137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataprep import *\n",
    "from OptimizedDataGeneratorNew import OptimizedDataGenerator\n",
    "from loss import *\n",
    "from models import *\n",
    "# import mdmm\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff499d81-383f-4988-94d7-12815179e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbc64d0-998a-4022-b4f5-e886c4c07941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/uscms/home/bweiss/nobackup/smart-pixels/\"\n",
    "tfrecords_base_dir = os.path.join(dataset_base_dir, \"tfrecords\")\n",
    "\n",
    "# dataset_dir_train = os.path.join(dataset_base_dir, \"dataset3src_50x12p5\", 'train')\n",
    "# dataset_dir_val = os.path.join(dataset_base_dir, \"dataset3src_50x12p5\", 'test')\n",
    "\n",
    "# tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\",'3src_80eNoise')\n",
    "# tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\",'3src_80eNoise')\n",
    "\n",
    "dataset_dir_train = os.path.join(dataset_base_dir, \"dataset_2s_50x12P5_parquets\", 'shuffled')\n",
    "# dataset_dir_val = os.path.join(dataset_base_dir, \"dataset3src_50x12p5\", 'test')\n",
    "\n",
    "tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\",'dataset_2s_50x12P5_OG_shuffled')\n",
    "tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\",'dataset_2s_50x12P5_OG_shuffled')\n",
    "\n",
    "\n",
    "# tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\",'2TS_OG')\n",
    "# tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\",'2TS_OG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c17a48-9308-40e4-a126-5478659cfedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(dataset_dir_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8bb9b7-c75e-489d-83bc-ad517c652aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = 80\n",
    "val_file_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66491533-15ba-4337-917e-8f99508cf3bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /uscms/home/bweiss/nobackup/smart-pixels/tfrecords/TFR_val/3src_80eNoise does not exist and cannot be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 20/20 [00:04<00:00,  4.07it/s]\n",
      "Saving batches as TFRecords: 100%|██████████| 23/23 [00:08<00:00,  2.59it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation generator 14.216791868209839 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_dir_val,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, \n",
    "    files_from_end=True,\n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = [0, 19], #-1\n",
    "    noise = [0,80],\n",
    "    max_workers = 2\n",
    ")\n",
    "\n",
    "print(\"--- Validation generator %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# validation_generator.debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dccba057-7aa7-4b85-95bd-8977c14cf7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /uscms/home/bweiss/nobackup/smart-pixels/tfrecords/TFR_train/3src_80eNoise does not exist and cannot be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 80/80 [00:17<00:00,  4.61it/s]\n",
      "Saving batches as TFRecords: 100%|██████████| 93/93 [00:37<00:00,  2.50it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training generator 55.177582025527954 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# training generator\n",
    "start_time = time.time()\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_dir_train,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, # True \n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = [0, 19], #-1\n",
    "    noise = [0,80],\n",
    "    max_workers = 2\n",
    ")\n",
    "print(\"--- Training generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248e5d17-2a8f-4e8c-871a-65a0ab3794b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n",
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n"
     ]
    }
   ],
   "source": [
    "training_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train,\n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")\n",
    "\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_val,\n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9b8ee3-f02f-4a0a-af31-4cfa3ccdaef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 02:08:37.834928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.583330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.583616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.584442: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-15 02:08:38.592232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.592433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.592589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.645320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.645562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.645715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-15 02:08:38.645859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38660 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe MIG 4g.40gb, pci bus id: 0000:00:11.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f46302e4880>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "cluster = training_generator[0]\n",
    "cluster=cluster[0][10,:,:,-1]\n",
    "plt.imshow(cluster)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72cef1b-61db-489f-83bb-039a46861094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 13, 21, 2)]       0         \n",
      "                                                                 \n",
      " q_separable_conv2d_1 (QSepa  (None, 11, 19, 5)        33        \n",
      " rableConv2D)                                                    \n",
      "                                                                 \n",
      " q_activation_5 (QActivation  (None, 11, 19, 5)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " q_conv2d_1 (QConv2D)        (None, 11, 19, 5)         30        \n",
      "                                                                 \n",
      " q_activation_6 (QActivation  (None, 11, 19, 5)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 3, 6, 5)          0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " q_activation_7 (QActivation  (None, 3, 6, 5)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 90)                0         \n",
      "                                                                 \n",
      " q_dense_3 (QDense)          (None, 16)                1456      \n",
      "                                                                 \n",
      " q_activation_8 (QActivation  (None, 16)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " q_dense_4 (QDense)          (None, 16)                272       \n",
      "                                                                 \n",
      " q_activation_9 (QActivation  (None, 16)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " q_dense_5 (QDense)          (None, 14)                238       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,029\n",
      "Trainable params: 2,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CreateModel((13,21,2),n_filters=5,pool_size=3, #mean_filter=3, thresh=0.6290114754408346\n",
    "                 )\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce60b630-5e1e-41cd-b9a0-7e9a06e9a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "training_name = '80eNoise'\n",
    "base_dir = f'./trained_models/model-{fingerprint}-{training_name}-checkpoints'\n",
    "os.makedirs(base_dir, exist_ok=True)  \n",
    "checkpoint_filepath = base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ccc68b0-0d5a-4b14-bebc-0d8e6923e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a86fd471\n"
     ]
    }
   ],
   "source": [
    "print(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd97cbbf-a137-4a5c-9796-11de00abfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "early_stopping_patience = 50\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        checkpoints = [f for f in os.listdir(base_dir) if f.startswith('weights')]\n",
    "        # .sort() does not order weight file strings numerically\n",
    "        if len(checkpoints) > 1:\n",
    "            checkpoints.sort(key=lambda x: os.path.getmtime(os.path.join(base_dir,x)))\n",
    "            for checkpoint in checkpoints[:-1]:\n",
    "                os.remove(os.path.join(base_dir, checkpoint))\n",
    "\n",
    "es = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "mcp = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(f'{base_dir}/training_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 02:11:13.866170: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-05-15 02:11:14.000326: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8800\n",
      "2025-05-15 02:11:14.128220: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x5595096dffa0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/93 [============================>.] - ETA: 0s - loss: 35621.5195\n",
      "Epoch 1: val_loss improved from inf to 18174.10156, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.01-t35617.07-v18174.10.hdf5\n",
      "93/93 [==============================] - 15s 124ms/step - loss: 35617.0742 - val_loss: 18174.1016\n",
      "Epoch 2/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 16843.2383\n",
      "Epoch 2: val_loss improved from 18174.10156 to 15926.72363, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.02-t16843.24-v15926.72.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 16843.2383 - val_loss: 15926.7236\n",
      "Epoch 3/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 15493.5967\n",
      "Epoch 3: val_loss improved from 15926.72363 to 14994.98535, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.03-t15493.60-v14994.99.hdf5\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 15493.5967 - val_loss: 14994.9854\n",
      "Epoch 4/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 14659.4844\n",
      "Epoch 4: val_loss improved from 14994.98535 to 14227.62598, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.04-t14623.03-v14227.63.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 14623.0342 - val_loss: 14227.6260\n",
      "Epoch 5/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 14110.2646\n",
      "Epoch 5: val_loss improved from 14227.62598 to 13757.82129, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.05-t14110.26-v13757.82.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 14110.2646 - val_loss: 13757.8213\n",
      "Epoch 6/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 13662.3389\n",
      "Epoch 6: val_loss improved from 13757.82129 to 13721.20410, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.06-t13662.34-v13721.20.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 13662.3389 - val_loss: 13721.2041\n",
      "Epoch 7/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 13426.2637\n",
      "Epoch 7: val_loss improved from 13721.20410 to 13123.94043, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.07-t13379.25-v13123.94.hdf5\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 13379.2520 - val_loss: 13123.9404\n",
      "Epoch 8/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 13081.1836\n",
      "Epoch 8: val_loss improved from 13123.94043 to 12830.28613, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.08-t13037.42-v12830.29.hdf5\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 13037.4199 - val_loss: 12830.2861\n",
      "Epoch 9/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12862.8779\n",
      "Epoch 9: val_loss improved from 12830.28613 to 12692.86621, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.09-t12818.16-v12692.87.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 12818.1621 - val_loss: 12692.8662\n",
      "Epoch 10/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12769.6426\n",
      "Epoch 10: val_loss improved from 12692.86621 to 12661.42676, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.10-t12769.64-v12661.43.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12769.6426 - val_loss: 12661.4268\n",
      "Epoch 11/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12617.4346\n",
      "Epoch 11: val_loss did not improve from 12661.42676\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12701.7070 - val_loss: 12689.3604\n",
      "Epoch 12/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12584.7578\n",
      "Epoch 12: val_loss improved from 12661.42676 to 12495.01270, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.12-t12608.32-v12495.01.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12608.3184 - val_loss: 12495.0127\n",
      "Epoch 13/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12547.5117\n",
      "Epoch 13: val_loss improved from 12495.01270 to 12466.26270, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.13-t12547.51-v12466.26.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12547.5117 - val_loss: 12466.2627\n",
      "Epoch 14/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12492.6338\n",
      "Epoch 14: val_loss did not improve from 12466.26270\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12492.6338 - val_loss: 12729.5068\n",
      "Epoch 15/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12506.6855\n",
      "Epoch 15: val_loss did not improve from 12466.26270\n",
      "93/93 [==============================] - 6s 61ms/step - loss: 12490.2969 - val_loss: 12580.3789\n",
      "Epoch 16/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12471.7422\n",
      "Epoch 16: val_loss improved from 12466.26270 to 12327.23535, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.16-t12436.70-v12327.24.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 12436.6963 - val_loss: 12327.2354\n",
      "Epoch 17/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12438.8525\n",
      "Epoch 17: val_loss improved from 12327.23535 to 12280.72656, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.17-t12396.59-v12280.73.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12396.5889 - val_loss: 12280.7266\n",
      "Epoch 18/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12412.9893\n",
      "Epoch 18: val_loss did not improve from 12280.72656\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 12412.9893 - val_loss: 12323.6885\n",
      "Epoch 19/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12408.2793\n",
      "Epoch 19: val_loss did not improve from 12280.72656\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 12408.2793 - val_loss: 12407.8525\n",
      "Epoch 20/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12363.7314\n",
      "Epoch 20: val_loss did not improve from 12280.72656\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 12363.7314 - val_loss: 12505.6328\n",
      "Epoch 21/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12409.3750\n",
      "Epoch 21: val_loss improved from 12280.72656 to 12241.17871, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.21-t12381.71-v12241.18.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 12381.7109 - val_loss: 12241.1787\n",
      "Epoch 22/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12357.7412\n",
      "Epoch 22: val_loss did not improve from 12241.17871\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 12349.1973 - val_loss: 12285.3750\n",
      "Epoch 23/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12336.3799\n",
      "Epoch 23: val_loss did not improve from 12241.17871\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 12336.3799 - val_loss: 12482.7158\n",
      "Epoch 24/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12251.9072\n",
      "Epoch 24: val_loss did not improve from 12241.17871\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12331.7764 - val_loss: 12334.8271\n",
      "Epoch 25/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12298.6543\n",
      "Epoch 25: val_loss improved from 12241.17871 to 12155.10449, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.25-t12298.65-v12155.10.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 12298.6543 - val_loss: 12155.1045\n",
      "Epoch 26/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12243.5596\n",
      "Epoch 26: val_loss improved from 12155.10449 to 12154.66895, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.26-t12228.94-v12154.67.hdf5\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 12228.9395 - val_loss: 12154.6689\n",
      "Epoch 27/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12234.3945\n",
      "Epoch 27: val_loss did not improve from 12154.66895\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 12234.3945 - val_loss: 12161.1875\n",
      "Epoch 28/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12234.5371\n",
      "Epoch 28: val_loss improved from 12154.66895 to 12124.47559, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.28-t12234.54-v12124.48.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12234.5371 - val_loss: 12124.4756\n",
      "Epoch 29/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12249.6201\n",
      "Epoch 29: val_loss did not improve from 12124.47559\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 12218.4668 - val_loss: 12151.0029\n",
      "Epoch 30/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12250.4072\n",
      "Epoch 30: val_loss improved from 12124.47559 to 12106.26953, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.30-t12211.75-v12106.27.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12211.7451 - val_loss: 12106.2695\n",
      "Epoch 31/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12240.8887\n",
      "Epoch 31: val_loss improved from 12106.26953 to 12090.36914, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.31-t12198.28-v12090.37.hdf5\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 12198.2803 - val_loss: 12090.3691\n",
      "Epoch 32/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12200.6016\n",
      "Epoch 32: val_loss did not improve from 12090.36914\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 12184.5752 - val_loss: 12095.0615\n",
      "Epoch 33/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12173.2363\n",
      "Epoch 33: val_loss did not improve from 12090.36914\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 12173.2363 - val_loss: 12194.8838\n",
      "Epoch 34/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12175.3740\n",
      "Epoch 34: val_loss improved from 12090.36914 to 12062.22656, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.34-t12175.37-v12062.23.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 12175.3740 - val_loss: 12062.2266\n",
      "Epoch 35/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12131.9658\n",
      "Epoch 35: val_loss improved from 12062.22656 to 12057.33594, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.35-t12131.97-v12057.34.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 12131.9658 - val_loss: 12057.3359\n",
      "Epoch 36/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12113.4355\n",
      "Epoch 36: val_loss did not improve from 12057.33594\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 12113.4355 - val_loss: 12132.7754\n",
      "Epoch 37/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12092.0430\n",
      "Epoch 37: val_loss improved from 12057.33594 to 12000.14551, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.37-t12089.43-v12000.15.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 12089.4316 - val_loss: 12000.1455\n",
      "Epoch 38/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12094.6865\n",
      "Epoch 38: val_loss improved from 12000.14551 to 11992.24609, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.38-t12067.72-v11992.25.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 12067.7188 - val_loss: 11992.2461\n",
      "Epoch 39/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12057.7285\n",
      "Epoch 39: val_loss did not improve from 11992.24609\n",
      "93/93 [==============================] - 6s 61ms/step - loss: 12057.7285 - val_loss: 11998.6963\n",
      "Epoch 40/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12032.8594\n",
      "Epoch 40: val_loss improved from 11992.24609 to 11970.93945, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.40-t12032.86-v11970.94.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 12032.8594 - val_loss: 11970.9395\n",
      "Epoch 41/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12033.2236\n",
      "Epoch 41: val_loss improved from 11970.93945 to 11954.73242, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.41-t12033.22-v11954.73.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 12033.2236 - val_loss: 11954.7324\n",
      "Epoch 42/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12011.4551\n",
      "Epoch 42: val_loss did not improve from 11954.73242\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 12011.4551 - val_loss: 11957.4844\n",
      "Epoch 43/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12052.1191\n",
      "Epoch 43: val_loss improved from 11954.73242 to 11944.79395, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.43-t12014.88-v11944.79.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 12014.8848 - val_loss: 11944.7939\n",
      "Epoch 44/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12009.8164\n",
      "Epoch 44: val_loss did not improve from 11944.79395\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 12009.8164 - val_loss: 12030.8809\n",
      "Epoch 45/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11997.5195\n",
      "Epoch 45: val_loss did not improve from 11944.79395\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 11997.5195 - val_loss: 12063.3359\n",
      "Epoch 46/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 12021.1279\n",
      "Epoch 46: val_loss improved from 11944.79395 to 11914.44824, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.46-t12021.13-v11914.45.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 12021.1279 - val_loss: 11914.4482\n",
      "Epoch 47/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12039.3145\n",
      "Epoch 47: val_loss improved from 11914.44824 to 11906.85938, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.47-t12005.65-v11906.86.hdf5\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 12005.6543 - val_loss: 11906.8594\n",
      "Epoch 48/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11999.8711\n",
      "Epoch 48: val_loss did not improve from 11906.85938\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 11999.8711 - val_loss: 11996.4297\n",
      "Epoch 49/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11990.2441\n",
      "Epoch 49: val_loss improved from 11906.85938 to 11896.11523, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.49-t11990.24-v11896.12.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 11990.2441 - val_loss: 11896.1152\n",
      "Epoch 50/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11956.7900\n",
      "Epoch 50: val_loss did not improve from 11896.11523\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 11956.7900 - val_loss: 11962.2959\n",
      "Epoch 51/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11963.2910\n",
      "Epoch 51: val_loss did not improve from 11896.11523\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 11963.2910 - val_loss: 12038.1064\n",
      "Epoch 52/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 12040.8584\n",
      "Epoch 52: val_loss did not improve from 11896.11523\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 11999.8975 - val_loss: 11951.6689\n",
      "Epoch 53/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11933.4062\n",
      "Epoch 53: val_loss improved from 11896.11523 to 11891.74609, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.53-t11933.41-v11891.75.hdf5\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 11933.4062 - val_loss: 11891.7461\n",
      "Epoch 54/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11885.9453\n",
      "Epoch 54: val_loss improved from 11891.74609 to 11783.21973, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.54-t11885.95-v11783.22.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 11885.9453 - val_loss: 11783.2197\n",
      "Epoch 55/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11863.6953\n",
      "Epoch 55: val_loss improved from 11783.21973 to 11777.71191, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.55-t11863.70-v11777.71.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 11863.6953 - val_loss: 11777.7119\n",
      "Epoch 56/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11843.2607\n",
      "Epoch 56: val_loss did not improve from 11777.71191\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 11843.2607 - val_loss: 11814.9053\n",
      "Epoch 57/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11830.7012\n",
      "Epoch 57: val_loss did not improve from 11777.71191\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 11830.7012 - val_loss: 11806.3799\n",
      "Epoch 58/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 11855.0322\n",
      "Epoch 58: val_loss did not improve from 11777.71191\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 11840.8613 - val_loss: 11778.6953\n",
      "Epoch 59/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11818.1074\n",
      "Epoch 59: val_loss improved from 11777.71191 to 11743.06836, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.59-t11818.11-v11743.07.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 11818.1074 - val_loss: 11743.0684\n",
      "Epoch 60/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11793.0449\n",
      "Epoch 60: val_loss improved from 11743.06836 to 11681.99805, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.60-t11793.04-v11682.00.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 11793.0449 - val_loss: 11681.9980\n",
      "Epoch 61/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11779.9209\n",
      "Epoch 61: val_loss improved from 11681.99805 to 11678.03223, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.61-t11779.92-v11678.03.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 11779.9209 - val_loss: 11678.0322\n",
      "Epoch 62/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11771.5000\n",
      "Epoch 62: val_loss did not improve from 11678.03223\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 11771.5000 - val_loss: 11775.1934\n",
      "Epoch 63/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11736.9902\n",
      "Epoch 63: val_loss improved from 11678.03223 to 11653.43066, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.63-t11736.99-v11653.43.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 11736.9902 - val_loss: 11653.4307\n",
      "Epoch 64/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 11693.9326\n",
      "Epoch 64: val_loss improved from 11653.43066 to 11600.26270, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.64-t11686.60-v11600.26.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 11686.5957 - val_loss: 11600.2627\n",
      "Epoch 65/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11621.1758\n",
      "Epoch 65: val_loss improved from 11600.26270 to 11527.16699, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.65-t11621.18-v11527.17.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 11621.1758 - val_loss: 11527.1670\n",
      "Epoch 66/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11500.5527\n",
      "Epoch 66: val_loss improved from 11527.16699 to 11352.73047, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.66-t11500.55-v11352.73.hdf5\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 11500.5527 - val_loss: 11352.7305\n",
      "Epoch 67/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11373.3828\n",
      "Epoch 67: val_loss improved from 11352.73047 to 11254.76172, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.67-t11373.38-v11254.76.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 11373.3828 - val_loss: 11254.7617\n",
      "Epoch 68/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 11339.9375\n",
      "Epoch 68: val_loss improved from 11254.76172 to 11216.46289, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.68-t11298.78-v11216.46.hdf5\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 11298.7793 - val_loss: 11216.4629\n",
      "Epoch 69/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 11343.5869\n",
      "Epoch 69: val_loss did not improve from 11216.46289\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 11328.7178 - val_loss: 11753.0400\n",
      "Epoch 70/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11226.3271\n",
      "Epoch 70: val_loss improved from 11216.46289 to 11048.09375, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.70-t11226.33-v11048.09.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 11226.3271 - val_loss: 11048.0938\n",
      "Epoch 71/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 11183.1797\n",
      "Epoch 71: val_loss improved from 11048.09375 to 10953.55566, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.71-t11148.83-v10953.56.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 11148.8330 - val_loss: 10953.5557\n",
      "Epoch 72/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11079.9795\n",
      "Epoch 72: val_loss did not improve from 10953.55566\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 11079.9795 - val_loss: 11006.8906\n",
      "Epoch 73/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11027.2988\n",
      "Epoch 73: val_loss improved from 10953.55566 to 10862.56445, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.73-t11027.30-v10862.56.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 11027.2988 - val_loss: 10862.5645\n",
      "Epoch 74/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11026.5332\n",
      "Epoch 74: val_loss did not improve from 10862.56445\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 11026.5332 - val_loss: 10862.6992\n",
      "Epoch 75/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 11011.0439\n",
      "Epoch 75: val_loss improved from 10862.56445 to 10860.64941, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.75-t11011.04-v10860.65.hdf5\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 11011.0439 - val_loss: 10860.6494\n",
      "Epoch 76/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10987.1318\n",
      "Epoch 76: val_loss did not improve from 10860.64941\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 10987.1318 - val_loss: 10886.5205\n",
      "Epoch 77/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10976.3154\n",
      "Epoch 77: val_loss improved from 10860.64941 to 10806.08887, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.77-t10946.86-v10806.09.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10946.8633 - val_loss: 10806.0889\n",
      "Epoch 78/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10909.3584\n",
      "Epoch 78: val_loss did not improve from 10806.08887\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10909.3584 - val_loss: 10858.8691\n",
      "Epoch 79/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10894.1084\n",
      "Epoch 79: val_loss improved from 10806.08887 to 10754.78223, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.79-t10894.11-v10754.78.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 10894.1084 - val_loss: 10754.7822\n",
      "Epoch 80/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10953.0166\n",
      "Epoch 80: val_loss did not improve from 10754.78223\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10931.0283 - val_loss: 10866.4238\n",
      "Epoch 81/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10879.7129\n",
      "Epoch 81: val_loss did not improve from 10754.78223\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10879.7129 - val_loss: 10783.2754\n",
      "Epoch 82/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10875.6621\n",
      "Epoch 82: val_loss did not improve from 10754.78223\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10875.6621 - val_loss: 10981.7656\n",
      "Epoch 83/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10842.6904\n",
      "Epoch 83: val_loss did not improve from 10754.78223\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10842.6904 - val_loss: 10865.2090\n",
      "Epoch 84/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10892.0557\n",
      "Epoch 84: val_loss improved from 10754.78223 to 10750.31055, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.84-t10852.38-v10750.31.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10852.3799 - val_loss: 10750.3105\n",
      "Epoch 85/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10816.9990\n",
      "Epoch 85: val_loss improved from 10750.31055 to 10646.70215, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.85-t10817.00-v10646.70.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10816.9990 - val_loss: 10646.7021\n",
      "Epoch 86/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10795.5537\n",
      "Epoch 86: val_loss did not improve from 10646.70215\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10795.5537 - val_loss: 10783.2676\n",
      "Epoch 87/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10861.0693\n",
      "Epoch 87: val_loss did not improve from 10646.70215\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10839.3145 - val_loss: 10778.4814\n",
      "Epoch 88/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10821.1133\n",
      "Epoch 88: val_loss improved from 10646.70215 to 10614.45605, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.88-t10821.11-v10614.46.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10821.1133 - val_loss: 10614.4561\n",
      "Epoch 89/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10793.9785\n",
      "Epoch 89: val_loss did not improve from 10614.45605\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10793.9785 - val_loss: 10821.8398\n",
      "Epoch 90/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10791.9961\n",
      "Epoch 90: val_loss did not improve from 10614.45605\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10775.0547 - val_loss: 10941.6152\n",
      "Epoch 91/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10789.7197\n",
      "Epoch 91: val_loss did not improve from 10614.45605\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10762.3369 - val_loss: 10659.5176\n",
      "Epoch 92/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10723.1777\n",
      "Epoch 92: val_loss did not improve from 10614.45605\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10723.1777 - val_loss: 10784.7930\n",
      "Epoch 93/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10747.5303\n",
      "Epoch 93: val_loss improved from 10614.45605 to 10518.15332, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.93-t10714.82-v10518.15.hdf5\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 10714.8174 - val_loss: 10518.1533\n",
      "Epoch 94/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10714.7725\n",
      "Epoch 94: val_loss did not improve from 10518.15332\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10713.4404 - val_loss: 10690.0195\n",
      "Epoch 95/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10736.7090\n",
      "Epoch 95: val_loss did not improve from 10518.15332\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10707.7441 - val_loss: 10537.8467\n",
      "Epoch 96/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10680.1484\n",
      "Epoch 96: val_loss improved from 10518.15332 to 10484.98047, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.96-t10680.15-v10484.98.hdf5\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10680.1484 - val_loss: 10484.9805\n",
      "Epoch 97/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10660.4326\n",
      "Epoch 97: val_loss improved from 10484.98047 to 10416.30078, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.97-t10660.43-v10416.30.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10660.4326 - val_loss: 10416.3008\n",
      "Epoch 98/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10646.7383\n",
      "Epoch 98: val_loss did not improve from 10416.30078\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10615.4141 - val_loss: 10553.5703\n",
      "Epoch 99/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10631.2305\n",
      "Epoch 99: val_loss did not improve from 10416.30078\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10599.3457 - val_loss: 10621.1914\n",
      "Epoch 100/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10604.5986\n",
      "Epoch 100: val_loss did not improve from 10416.30078\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 10604.5986 - val_loss: 10421.8252\n",
      "Epoch 101/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10624.1279\n",
      "Epoch 101: val_loss improved from 10416.30078 to 10357.19824, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.101-t10607.01-v10357.20.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10607.0137 - val_loss: 10357.1982\n",
      "Epoch 102/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10567.1846\n",
      "Epoch 102: val_loss did not improve from 10357.19824\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10567.1846 - val_loss: 10396.8877\n",
      "Epoch 103/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10614.0410\n",
      "Epoch 103: val_loss did not improve from 10357.19824\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10578.4971 - val_loss: 10370.5303\n",
      "Epoch 104/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10495.9053\n",
      "Epoch 104: val_loss did not improve from 10357.19824\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 10495.9053 - val_loss: 10397.8906\n",
      "Epoch 105/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10480.9121\n",
      "Epoch 105: val_loss improved from 10357.19824 to 10277.43359, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.105-t10444.49-v10277.43.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10444.4883 - val_loss: 10277.4336\n",
      "Epoch 106/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10447.8096\n",
      "Epoch 106: val_loss did not improve from 10277.43359\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 10447.8096 - val_loss: 10376.8877\n",
      "Epoch 107/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10416.1689\n",
      "Epoch 107: val_loss did not improve from 10277.43359\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10416.1689 - val_loss: 10451.7539\n",
      "Epoch 108/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10390.3584\n",
      "Epoch 108: val_loss did not improve from 10277.43359\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10390.3584 - val_loss: 10345.5264\n",
      "Epoch 109/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10414.6133\n",
      "Epoch 109: val_loss improved from 10277.43359 to 10259.29883, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.109-t10414.61-v10259.30.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10414.6133 - val_loss: 10259.2988\n",
      "Epoch 110/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10420.8418\n",
      "Epoch 110: val_loss did not improve from 10259.29883\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10394.2852 - val_loss: 10291.2549\n",
      "Epoch 111/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10422.5029\n",
      "Epoch 111: val_loss did not improve from 10259.29883\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10422.5029 - val_loss: 10498.6904\n",
      "Epoch 112/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10435.3623\n",
      "Epoch 112: val_loss improved from 10259.29883 to 10225.86133, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.112-t10396.90-v10225.86.hdf5\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10396.8994 - val_loss: 10225.8613\n",
      "Epoch 113/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10416.6201\n",
      "Epoch 113: val_loss did not improve from 10225.86133\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10379.5039 - val_loss: 10258.9385\n",
      "Epoch 114/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10380.5938\n",
      "Epoch 114: val_loss did not improve from 10225.86133\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 10380.5938 - val_loss: 10309.9229\n",
      "Epoch 115/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10381.9443\n",
      "Epoch 115: val_loss improved from 10225.86133 to 10219.60840, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.115-t10374.41-v10219.61.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10374.4131 - val_loss: 10219.6084\n",
      "Epoch 116/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10359.3984\n",
      "Epoch 116: val_loss improved from 10219.60840 to 10196.70312, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.116-t10359.40-v10196.70.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10359.3984 - val_loss: 10196.7031\n",
      "Epoch 117/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10372.6582\n",
      "Epoch 117: val_loss did not improve from 10196.70312\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 10342.3213 - val_loss: 10198.0137\n",
      "Epoch 118/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10331.3086\n",
      "Epoch 118: val_loss did not improve from 10196.70312\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10307.4326 - val_loss: 10282.1865\n",
      "Epoch 119/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10317.7217\n",
      "Epoch 119: val_loss did not improve from 10196.70312\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 10317.7217 - val_loss: 10284.9766\n",
      "Epoch 120/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10327.6719\n",
      "Epoch 120: val_loss did not improve from 10196.70312\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 10327.6719 - val_loss: 10329.6514\n",
      "Epoch 121/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10318.3984\n",
      "Epoch 121: val_loss improved from 10196.70312 to 10195.35840, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.121-t10313.34-v10195.36.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10313.3389 - val_loss: 10195.3584\n",
      "Epoch 122/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10299.1738\n",
      "Epoch 122: val_loss did not improve from 10195.35840\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10299.1738 - val_loss: 10338.5547\n",
      "Epoch 123/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10292.7451\n",
      "Epoch 123: val_loss did not improve from 10195.35840\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10292.7451 - val_loss: 10498.6670\n",
      "Epoch 124/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10289.7236\n",
      "Epoch 124: val_loss improved from 10195.35840 to 10069.70215, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.124-t10252.20-v10069.70.hdf5\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10252.1992 - val_loss: 10069.7021\n",
      "Epoch 125/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10287.4570\n",
      "Epoch 125: val_loss did not improve from 10069.70215\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10255.9922 - val_loss: 10211.9258\n",
      "Epoch 126/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10243.5146\n",
      "Epoch 126: val_loss did not improve from 10069.70215\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 10243.5146 - val_loss: 10231.3750\n",
      "Epoch 127/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10198.8984\n",
      "Epoch 127: val_loss did not improve from 10069.70215\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 10198.8984 - val_loss: 10082.2881\n",
      "Epoch 128/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10235.6035\n",
      "Epoch 128: val_loss did not improve from 10069.70215\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10199.0654 - val_loss: 10178.5225\n",
      "Epoch 129/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10157.3350\n",
      "Epoch 129: val_loss did not improve from 10069.70215\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10157.3350 - val_loss: 10263.2930\n",
      "Epoch 130/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10195.7930\n",
      "Epoch 130: val_loss improved from 10069.70215 to 10051.23438, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.130-t10195.79-v10051.23.hdf5\n",
      "93/93 [==============================] - 7s 73ms/step - loss: 10195.7930 - val_loss: 10051.2344\n",
      "Epoch 131/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10255.2627\n",
      "Epoch 131: val_loss improved from 10051.23438 to 10050.29004, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.131-t10255.26-v10050.29.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10255.2627 - val_loss: 10050.2900\n",
      "Epoch 132/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10181.3496\n",
      "Epoch 132: val_loss did not improve from 10050.29004\n",
      "93/93 [==============================] - 7s 74ms/step - loss: 10181.3496 - val_loss: 10087.9570\n",
      "Epoch 133/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10191.7441\n",
      "Epoch 133: val_loss did not improve from 10050.29004\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10191.7441 - val_loss: 10206.6152\n",
      "Epoch 134/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10172.8242\n",
      "Epoch 134: val_loss did not improve from 10050.29004\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10172.8242 - val_loss: 10148.3945\n",
      "Epoch 135/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10164.0439\n",
      "Epoch 135: val_loss did not improve from 10050.29004\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10164.0439 - val_loss: 10340.1934\n",
      "Epoch 136/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10175.3750\n",
      "Epoch 136: val_loss did not improve from 10050.29004\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10162.0527 - val_loss: 10203.9150\n",
      "Epoch 137/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10172.8955\n",
      "Epoch 137: val_loss improved from 10050.29004 to 10002.86816, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.137-t10172.90-v10002.87.hdf5\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10172.8955 - val_loss: 10002.8682\n",
      "Epoch 138/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10177.0244\n",
      "Epoch 138: val_loss did not improve from 10002.86816\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10163.6729 - val_loss: 10325.9639\n",
      "Epoch 139/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10185.0137\n",
      "Epoch 139: val_loss did not improve from 10002.86816\n",
      "93/93 [==============================] - 7s 72ms/step - loss: 10216.3232 - val_loss: 10164.6514\n",
      "Epoch 140/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10181.9570\n",
      "Epoch 140: val_loss did not improve from 10002.86816\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10144.4219 - val_loss: 10079.5156\n",
      "Epoch 141/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10143.7607\n",
      "Epoch 141: val_loss improved from 10002.86816 to 9923.10742, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.141-t10143.76-v9923.11.hdf5\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10143.7607 - val_loss: 9923.1074\n",
      "Epoch 142/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10148.4619\n",
      "Epoch 142: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10142.0869 - val_loss: 10056.4980\n",
      "Epoch 143/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10116.7930\n",
      "Epoch 143: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10116.7930 - val_loss: 10111.9834\n",
      "Epoch 144/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10157.5859\n",
      "Epoch 144: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 7s 72ms/step - loss: 10144.2324 - val_loss: 9937.9121\n",
      "Epoch 145/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10117.5488\n",
      "Epoch 145: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10117.5488 - val_loss: 10042.6934\n",
      "Epoch 146/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10095.3193\n",
      "Epoch 146: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10060.8545 - val_loss: 9982.8213\n",
      "Epoch 147/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10066.6064\n",
      "Epoch 147: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10066.6064 - val_loss: 10001.0273\n",
      "Epoch 148/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10051.9492\n",
      "Epoch 148: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10051.9492 - val_loss: 10020.1729\n",
      "Epoch 149/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10042.6104\n",
      "Epoch 149: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10042.6104 - val_loss: 10010.0898\n",
      "Epoch 150/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10030.4346\n",
      "Epoch 150: val_loss did not improve from 9923.10742\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10030.4346 - val_loss: 10121.7715\n",
      "Epoch 151/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10038.7812\n",
      "Epoch 151: val_loss improved from 9923.10742 to 9905.94336, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.151-t10038.78-v9905.94.hdf5\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10038.7812 - val_loss: 9905.9434\n",
      "Epoch 152/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10027.7881\n",
      "Epoch 152: val_loss improved from 9905.94336 to 9859.04004, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.152-t10027.79-v9859.04.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10027.7881 - val_loss: 9859.0400\n",
      "Epoch 153/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10031.1680\n",
      "Epoch 153: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 10031.1680 - val_loss: 10002.8301\n",
      "Epoch 154/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10025.8037\n",
      "Epoch 154: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10025.8037 - val_loss: 9961.1689\n",
      "Epoch 155/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10062.2910\n",
      "Epoch 155: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10054.8408 - val_loss: 9970.7998\n",
      "Epoch 156/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10028.7891\n",
      "Epoch 156: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 10028.7891 - val_loss: 10017.4404\n",
      "Epoch 157/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10068.2852\n",
      "Epoch 157: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10068.2852 - val_loss: 10087.3242\n",
      "Epoch 158/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10064.0049\n",
      "Epoch 158: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10062.7559 - val_loss: 10340.3789\n",
      "Epoch 159/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10053.5576\n",
      "Epoch 159: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 10053.5576 - val_loss: 9952.6445\n",
      "Epoch 160/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10059.8906\n",
      "Epoch 160: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10059.8906 - val_loss: 10172.2988\n",
      "Epoch 161/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10102.8232\n",
      "Epoch 161: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10089.6025 - val_loss: 10407.0498\n",
      "Epoch 162/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10100.8809\n",
      "Epoch 162: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 10100.8809 - val_loss: 10173.7256\n",
      "Epoch 163/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10074.3389\n",
      "Epoch 163: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10074.3389 - val_loss: 10379.8359\n",
      "Epoch 164/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10082.5811\n",
      "Epoch 164: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10082.5811 - val_loss: 9896.3301\n",
      "Epoch 165/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10043.2891\n",
      "Epoch 165: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 10043.2891 - val_loss: 10370.8467\n",
      "Epoch 166/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10053.9160\n",
      "Epoch 166: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 10053.9160 - val_loss: 10140.0557\n",
      "Epoch 167/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10045.4092\n",
      "Epoch 167: val_loss did not improve from 9859.04004\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10045.4092 - val_loss: 10034.3809\n",
      "Epoch 168/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10027.6885\n",
      "Epoch 168: val_loss improved from 9859.04004 to 9815.40625, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.168-t10027.69-v9815.41.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10027.6885 - val_loss: 9815.4062\n",
      "Epoch 169/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10038.7559\n",
      "Epoch 169: val_loss improved from 9815.40625 to 9802.17285, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.169-t10004.35-v9802.17.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 10004.3545 - val_loss: 9802.1729\n",
      "Epoch 170/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10005.3271\n",
      "Epoch 170: val_loss did not improve from 9802.17285\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9969.9580 - val_loss: 9910.3838\n",
      "Epoch 171/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10020.8838\n",
      "Epoch 171: val_loss did not improve from 9802.17285\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9986.1357 - val_loss: 9882.7666\n",
      "Epoch 172/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10006.2988\n",
      "Epoch 172: val_loss did not improve from 9802.17285\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 9971.1885 - val_loss: 10003.4229\n",
      "Epoch 173/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10038.4053\n",
      "Epoch 173: val_loss did not improve from 9802.17285\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 10004.4551 - val_loss: 9806.6689\n",
      "Epoch 174/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9993.2354 \n",
      "Epoch 174: val_loss did not improve from 9802.17285\n",
      "93/93 [==============================] - 7s 72ms/step - loss: 9993.2354 - val_loss: 9843.0010\n",
      "Epoch 175/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9924.4854\n",
      "Epoch 175: val_loss improved from 9802.17285 to 9753.27539, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.175-t9924.49-v9753.28.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9924.4854 - val_loss: 9753.2754\n",
      "Epoch 176/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9922.4883\n",
      "Epoch 176: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9943.3232 - val_loss: 9830.3242\n",
      "Epoch 177/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9961.8525\n",
      "Epoch 177: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9931.2871 - val_loss: 9844.9521\n",
      "Epoch 178/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9925.0537\n",
      "Epoch 178: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9925.0537 - val_loss: 9769.6914\n",
      "Epoch 179/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9946.0156\n",
      "Epoch 179: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9926.0674 - val_loss: 9767.0713\n",
      "Epoch 180/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9936.5820\n",
      "Epoch 180: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9926.3105 - val_loss: 9962.7441\n",
      "Epoch 181/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9949.3027\n",
      "Epoch 181: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9949.3027 - val_loss: 9943.5049\n",
      "Epoch 182/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9932.1436\n",
      "Epoch 182: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 7s 72ms/step - loss: 9907.2148 - val_loss: 10309.8711\n",
      "Epoch 183/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9995.4795\n",
      "Epoch 183: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9995.4795 - val_loss: 9931.4004\n",
      "Epoch 184/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9962.1934 \n",
      "Epoch 184: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9962.1934 - val_loss: 9891.7910\n",
      "Epoch 185/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 10047.7617\n",
      "Epoch 185: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 10034.9326 - val_loss: 9763.8115\n",
      "Epoch 186/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10001.4688\n",
      "Epoch 186: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 10001.4688 - val_loss: 9837.8701\n",
      "Epoch 187/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9948.1475\n",
      "Epoch 187: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9948.1475 - val_loss: 10197.9863\n",
      "Epoch 188/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 10011.5527\n",
      "Epoch 188: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 10011.5527 - val_loss: 10109.7588\n",
      "Epoch 189/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9945.4209\n",
      "Epoch 189: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9945.4209 - val_loss: 10372.6543\n",
      "Epoch 190/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9966.3037\n",
      "Epoch 190: val_loss did not improve from 9753.27539\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 9966.3037 - val_loss: 10193.8164\n",
      "Epoch 191/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9938.5400\n",
      "Epoch 191: val_loss improved from 9753.27539 to 9704.53320, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.191-t9938.54-v9704.53.hdf5\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9938.5400 - val_loss: 9704.5332\n",
      "Epoch 192/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9970.5459\n",
      "Epoch 192: val_loss did not improve from 9704.53320\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 9943.6709 - val_loss: 9836.3848\n",
      "Epoch 193/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9963.8291\n",
      "Epoch 193: val_loss did not improve from 9704.53320\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9963.8291 - val_loss: 9806.6963\n",
      "Epoch 194/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9953.9521\n",
      "Epoch 194: val_loss did not improve from 9704.53320\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9922.4209 - val_loss: 10075.7959\n",
      "Epoch 195/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9930.3770\n",
      "Epoch 195: val_loss did not improve from 9704.53320\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9893.8799 - val_loss: 9748.0215\n",
      "Epoch 196/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9928.9834\n",
      "Epoch 196: val_loss did not improve from 9704.53320\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9928.9834 - val_loss: 9763.7285\n",
      "Epoch 197/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9978.7246 \n",
      "Epoch 197: val_loss improved from 9704.53320 to 9680.84082, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.197-t9948.11-v9680.84.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9948.1094 - val_loss: 9680.8408\n",
      "Epoch 198/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9973.7422 \n",
      "Epoch 198: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9973.7422 - val_loss: 9793.2207\n",
      "Epoch 199/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9892.6826\n",
      "Epoch 199: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9892.6826 - val_loss: 9832.4629\n",
      "Epoch 200/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9912.4766\n",
      "Epoch 200: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 9912.4766 - val_loss: 10037.3691\n",
      "Epoch 201/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9888.3740\n",
      "Epoch 201: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9888.3740 - val_loss: 9854.0195\n",
      "Epoch 202/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9951.1406\n",
      "Epoch 202: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9951.1406 - val_loss: 10257.0381\n",
      "Epoch 203/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9917.1836\n",
      "Epoch 203: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9917.1836 - val_loss: 9969.9062\n",
      "Epoch 204/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9893.9805\n",
      "Epoch 204: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9893.9805 - val_loss: 10523.1055\n",
      "Epoch 205/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9947.6377\n",
      "Epoch 205: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9947.6377 - val_loss: 10083.3857\n",
      "Epoch 206/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9903.4463\n",
      "Epoch 206: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9903.4463 - val_loss: 9718.2822\n",
      "Epoch 207/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9955.3633 \n",
      "Epoch 207: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9955.3633 - val_loss: 9754.8359\n",
      "Epoch 208/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9895.6514\n",
      "Epoch 208: val_loss did not improve from 9680.84082\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9895.6514 - val_loss: 9739.7178\n",
      "Epoch 209/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9920.1416\n",
      "Epoch 209: val_loss improved from 9680.84082 to 9678.97070, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.209-t9897.82-v9678.97.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9897.8193 - val_loss: 9678.9707\n",
      "Epoch 210/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9908.8672\n",
      "Epoch 210: val_loss did not improve from 9678.97070\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9908.8672 - val_loss: 10204.2686\n",
      "Epoch 211/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9923.5498\n",
      "Epoch 211: val_loss did not improve from 9678.97070\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9923.5498 - val_loss: 10182.4658\n",
      "Epoch 212/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9923.9180\n",
      "Epoch 212: val_loss did not improve from 9678.97070\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9923.9180 - val_loss: 9692.2812\n",
      "Epoch 213/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9903.3008\n",
      "Epoch 213: val_loss did not improve from 9678.97070\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9903.3008 - val_loss: 9843.8760\n",
      "Epoch 214/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9912.9727\n",
      "Epoch 214: val_loss did not improve from 9678.97070\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9912.9727 - val_loss: 9828.8438\n",
      "Epoch 215/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9869.5107\n",
      "Epoch 215: val_loss improved from 9678.97070 to 9615.89551, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.215-t9887.85-v9615.90.hdf5\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9887.8486 - val_loss: 9615.8955\n",
      "Epoch 216/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9826.1787\n",
      "Epoch 216: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9826.1787 - val_loss: 10031.1016\n",
      "Epoch 217/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9893.6768\n",
      "Epoch 217: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9893.6768 - val_loss: 9876.0264\n",
      "Epoch 218/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9935.6270\n",
      "Epoch 218: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9935.6270 - val_loss: 9680.8379\n",
      "Epoch 219/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9869.0127\n",
      "Epoch 219: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9869.0127 - val_loss: 9882.1279\n",
      "Epoch 220/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9884.3701\n",
      "Epoch 220: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9853.3857 - val_loss: 9864.6729\n",
      "Epoch 221/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9895.9912\n",
      "Epoch 221: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9895.9912 - val_loss: 9829.2197\n",
      "Epoch 222/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9857.9336\n",
      "Epoch 222: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9842.3818 - val_loss: 9825.1123\n",
      "Epoch 223/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9855.7676\n",
      "Epoch 223: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9836.4844 - val_loss: 9907.6123\n",
      "Epoch 224/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9885.9629\n",
      "Epoch 224: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9865.4717 - val_loss: 9637.3760\n",
      "Epoch 225/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9874.9697\n",
      "Epoch 225: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9874.9697 - val_loss: 9839.0068\n",
      "Epoch 226/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9871.8369\n",
      "Epoch 226: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9871.8369 - val_loss: 9739.6084\n",
      "Epoch 227/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9868.3857\n",
      "Epoch 227: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9868.3857 - val_loss: 10101.8389\n",
      "Epoch 228/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9831.4688\n",
      "Epoch 228: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9831.4688 - val_loss: 10113.8770\n",
      "Epoch 229/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9916.0283\n",
      "Epoch 229: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9882.1953 - val_loss: 9881.0928\n",
      "Epoch 230/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9838.1709\n",
      "Epoch 230: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9810.7324 - val_loss: 9629.0068\n",
      "Epoch 231/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9833.0166\n",
      "Epoch 231: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9833.0166 - val_loss: 10013.3252\n",
      "Epoch 232/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9830.2197\n",
      "Epoch 232: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9830.2197 - val_loss: 9701.4307\n",
      "Epoch 233/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9789.9160\n",
      "Epoch 233: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9789.9160 - val_loss: 9987.5215\n",
      "Epoch 234/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9810.3428\n",
      "Epoch 234: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9810.3428 - val_loss: 9722.3174\n",
      "Epoch 235/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9811.9014\n",
      "Epoch 235: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9811.9014 - val_loss: 9879.5439\n",
      "Epoch 236/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9827.9336\n",
      "Epoch 236: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9827.9336 - val_loss: 9705.4902\n",
      "Epoch 237/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9855.0332\n",
      "Epoch 237: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9844.8213 - val_loss: 9721.6367\n",
      "Epoch 238/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9833.7520\n",
      "Epoch 238: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9806.9336 - val_loss: 10195.4385\n",
      "Epoch 239/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9801.9463\n",
      "Epoch 239: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9801.9463 - val_loss: 9767.6416\n",
      "Epoch 240/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9802.5352\n",
      "Epoch 240: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9802.5352 - val_loss: 9616.8945\n",
      "Epoch 241/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9842.2959\n",
      "Epoch 241: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9842.2959 - val_loss: 9845.0176\n",
      "Epoch 242/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9765.6035\n",
      "Epoch 242: val_loss did not improve from 9615.89551\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9765.6035 - val_loss: 9769.9434\n",
      "Epoch 243/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9747.6543\n",
      "Epoch 243: val_loss improved from 9615.89551 to 9565.37891, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.243-t9747.65-v9565.38.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9747.6543 - val_loss: 9565.3789\n",
      "Epoch 244/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9774.9424\n",
      "Epoch 244: val_loss did not improve from 9565.37891\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9774.9424 - val_loss: 9848.4639\n",
      "Epoch 245/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9828.4023\n",
      "Epoch 245: val_loss improved from 9565.37891 to 9534.57324, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.245-t9792.85-v9534.57.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9792.8477 - val_loss: 9534.5732\n",
      "Epoch 246/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9762.7666\n",
      "Epoch 246: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9762.7666 - val_loss: 9595.0801\n",
      "Epoch 247/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9749.3789\n",
      "Epoch 247: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9749.3789 - val_loss: 9575.7764\n",
      "Epoch 248/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9779.5703\n",
      "Epoch 248: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9779.5703 - val_loss: 9561.4648\n",
      "Epoch 249/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9758.0674\n",
      "Epoch 249: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9758.0674 - val_loss: 9680.9316\n",
      "Epoch 250/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9779.5371\n",
      "Epoch 250: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9754.8652 - val_loss: 9602.7139\n",
      "Epoch 251/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9757.3779\n",
      "Epoch 251: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9757.3779 - val_loss: 9573.0283\n",
      "Epoch 252/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9805.9834\n",
      "Epoch 252: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9805.9834 - val_loss: 9545.5039\n",
      "Epoch 253/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9782.5107\n",
      "Epoch 253: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9754.2920 - val_loss: 10043.6436\n",
      "Epoch 254/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9793.4512\n",
      "Epoch 254: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9793.4512 - val_loss: 9742.8496\n",
      "Epoch 255/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9844.3916\n",
      "Epoch 255: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9814.1865 - val_loss: 9679.9268\n",
      "Epoch 256/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9783.6934\n",
      "Epoch 256: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9783.6934 - val_loss: 9659.4160\n",
      "Epoch 257/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9805.4004\n",
      "Epoch 257: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9805.4004 - val_loss: 9675.7383\n",
      "Epoch 258/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9811.1377\n",
      "Epoch 258: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9824.0166 - val_loss: 9783.5107\n",
      "Epoch 259/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9785.7412\n",
      "Epoch 259: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9785.7412 - val_loss: 9738.2861\n",
      "Epoch 260/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9794.1660\n",
      "Epoch 260: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9794.1660 - val_loss: 9653.2041\n",
      "Epoch 261/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9790.2559\n",
      "Epoch 261: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9790.2559 - val_loss: 10483.4980\n",
      "Epoch 262/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9814.1035\n",
      "Epoch 262: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9814.1035 - val_loss: 9569.1729\n",
      "Epoch 263/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9791.9902\n",
      "Epoch 263: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9790.7754 - val_loss: 9995.9844\n",
      "Epoch 264/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9735.8652\n",
      "Epoch 264: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9771.6855 - val_loss: 9785.9209\n",
      "Epoch 265/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9838.6455\n",
      "Epoch 265: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9807.5762 - val_loss: 9605.2900\n",
      "Epoch 266/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9759.0576\n",
      "Epoch 266: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9759.0576 - val_loss: 9809.7744\n",
      "Epoch 267/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9804.3633\n",
      "Epoch 267: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9804.3633 - val_loss: 9777.1377\n",
      "Epoch 268/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9748.4434\n",
      "Epoch 268: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9748.4434 - val_loss: 9927.5898\n",
      "Epoch 269/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9811.4951\n",
      "Epoch 269: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9811.4951 - val_loss: 9728.2979\n",
      "Epoch 270/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9792.7256\n",
      "Epoch 270: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9792.7256 - val_loss: 10407.5488\n",
      "Epoch 271/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9788.9297\n",
      "Epoch 271: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9788.9297 - val_loss: 9870.9248\n",
      "Epoch 272/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9791.4316\n",
      "Epoch 272: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9791.4316 - val_loss: 9554.5781\n",
      "Epoch 273/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9842.3604\n",
      "Epoch 273: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9821.1631 - val_loss: 9786.2344\n",
      "Epoch 274/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9763.4668\n",
      "Epoch 274: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9763.4668 - val_loss: 9632.4453\n",
      "Epoch 275/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9791.0771\n",
      "Epoch 275: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9791.0771 - val_loss: 9567.2686\n",
      "Epoch 276/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9795.8135\n",
      "Epoch 276: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 9795.8135 - val_loss: 9822.0898\n",
      "Epoch 277/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9790.8301\n",
      "Epoch 277: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 7s 71ms/step - loss: 9790.8301 - val_loss: 9631.0752\n",
      "Epoch 278/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9765.2695\n",
      "Epoch 278: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9765.2695 - val_loss: 9723.9912\n",
      "Epoch 279/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9741.7041\n",
      "Epoch 279: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9741.7041 - val_loss: 9572.9336\n",
      "Epoch 280/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9765.9072\n",
      "Epoch 280: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9755.2236 - val_loss: 9730.0137\n",
      "Epoch 281/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9720.2285\n",
      "Epoch 281: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9720.2285 - val_loss: 9760.3184\n",
      "Epoch 282/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9745.0283\n",
      "Epoch 282: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9745.0283 - val_loss: 9582.9648\n",
      "Epoch 283/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9762.3672\n",
      "Epoch 283: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9740.5527 - val_loss: 9739.2861\n",
      "Epoch 284/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9814.9570\n",
      "Epoch 284: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9809.2188 - val_loss: 9717.3027\n",
      "Epoch 285/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9800.6943\n",
      "Epoch 285: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9800.6943 - val_loss: 9670.7197\n",
      "Epoch 286/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9654.9268\n",
      "Epoch 286: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9654.9268 - val_loss: 9907.9268\n",
      "Epoch 287/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9631.0059\n",
      "Epoch 287: val_loss did not improve from 9534.57324\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9631.0059 - val_loss: 10193.4141\n",
      "Epoch 288/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9670.8770\n",
      "Epoch 288: val_loss improved from 9534.57324 to 9474.22266, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.288-t9634.92-v9474.22.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9634.9248 - val_loss: 9474.2227\n",
      "Epoch 289/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9651.3379\n",
      "Epoch 289: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9623.9209 - val_loss: 9701.1211\n",
      "Epoch 290/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9629.6279\n",
      "Epoch 290: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9629.6279 - val_loss: 9695.5479\n",
      "Epoch 291/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9629.5195\n",
      "Epoch 291: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9629.5195 - val_loss: 9582.9053\n",
      "Epoch 292/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9675.5908\n",
      "Epoch 292: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9675.5908 - val_loss: 9650.3213\n",
      "Epoch 293/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9592.9551\n",
      "Epoch 293: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9592.9551 - val_loss: 9537.3350\n",
      "Epoch 294/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9660.3350\n",
      "Epoch 294: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9660.3350 - val_loss: 9497.1465\n",
      "Epoch 295/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9604.7236\n",
      "Epoch 295: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9604.7236 - val_loss: 9705.3330\n",
      "Epoch 296/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9713.1699\n",
      "Epoch 296: val_loss did not improve from 9474.22266\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9678.7197 - val_loss: 9680.5801\n",
      "Epoch 297/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9650.1562\n",
      "Epoch 297: val_loss improved from 9474.22266 to 9465.74512, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.297-t9650.16-v9465.75.hdf5\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9650.1562 - val_loss: 9465.7451\n",
      "Epoch 298/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9709.5479\n",
      "Epoch 298: val_loss improved from 9465.74512 to 9440.44531, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.298-t9679.71-v9440.45.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9679.7090 - val_loss: 9440.4453\n",
      "Epoch 299/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9699.8838\n",
      "Epoch 299: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9699.8838 - val_loss: 9661.7246\n",
      "Epoch 300/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9700.3447\n",
      "Epoch 300: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9700.3447 - val_loss: 9628.4932\n",
      "Epoch 301/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9701.5840\n",
      "Epoch 301: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9701.5840 - val_loss: 9546.1377\n",
      "Epoch 302/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9654.3057\n",
      "Epoch 302: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9654.3057 - val_loss: 9668.3604\n",
      "Epoch 303/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9700.4287\n",
      "Epoch 303: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9685.7979 - val_loss: 9461.1465\n",
      "Epoch 304/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9656.8076\n",
      "Epoch 304: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9656.8076 - val_loss: 9793.9004\n",
      "Epoch 305/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9712.0127\n",
      "Epoch 305: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9712.0127 - val_loss: 9469.8867\n",
      "Epoch 306/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9701.2959\n",
      "Epoch 306: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9668.7510 - val_loss: 9528.0596\n",
      "Epoch 307/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9701.5137\n",
      "Epoch 307: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9701.5137 - val_loss: 9698.5205\n",
      "Epoch 308/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9633.2920\n",
      "Epoch 308: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9633.2920 - val_loss: 10167.3037\n",
      "Epoch 309/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9613.0869\n",
      "Epoch 309: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9613.0869 - val_loss: 9812.2422\n",
      "Epoch 310/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9597.7930\n",
      "Epoch 310: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9597.7930 - val_loss: 9537.6357\n",
      "Epoch 311/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9546.3916\n",
      "Epoch 311: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9546.3916 - val_loss: 9519.5020\n",
      "Epoch 312/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9612.0928\n",
      "Epoch 312: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9599.3232 - val_loss: 9459.5684\n",
      "Epoch 313/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9608.3438\n",
      "Epoch 313: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9581.7832 - val_loss: 9461.2393\n",
      "Epoch 314/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9572.2598\n",
      "Epoch 314: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9572.2598 - val_loss: 9549.2852\n",
      "Epoch 315/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9596.6670\n",
      "Epoch 315: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9561.6553 - val_loss: 9545.4033\n",
      "Epoch 316/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9554.3740\n",
      "Epoch 316: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9554.3740 - val_loss: 9525.9238\n",
      "Epoch 317/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9603.4395\n",
      "Epoch 317: val_loss did not improve from 9440.44531\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9574.4717 - val_loss: 9668.8916\n",
      "Epoch 318/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9633.3389\n",
      "Epoch 318: val_loss improved from 9440.44531 to 9419.26953, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.318-t9605.44-v9419.27.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9605.4365 - val_loss: 9419.2695\n",
      "Epoch 319/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9595.6191\n",
      "Epoch 319: val_loss did not improve from 9419.26953\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9569.8828 - val_loss: 9703.5303\n",
      "Epoch 320/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9650.8818\n",
      "Epoch 320: val_loss did not improve from 9419.26953\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9650.8818 - val_loss: 9513.9590\n",
      "Epoch 321/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9664.8369\n",
      "Epoch 321: val_loss did not improve from 9419.26953\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9637.3818 - val_loss: 10140.4209\n",
      "Epoch 322/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9604.1611\n",
      "Epoch 322: val_loss did not improve from 9419.26953\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9591.9141 - val_loss: 9488.1172\n",
      "Epoch 323/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9602.3096\n",
      "Epoch 323: val_loss did not improve from 9419.26953\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9602.3096 - val_loss: 9500.5996\n",
      "Epoch 324/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9567.3760\n",
      "Epoch 324: val_loss did not improve from 9419.26953\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9555.2324 - val_loss: 9727.6709\n",
      "Epoch 325/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9589.1484\n",
      "Epoch 325: val_loss did not improve from 9419.26953\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9553.8701 - val_loss: 9461.8154\n",
      "Epoch 326/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9589.4004\n",
      "Epoch 326: val_loss improved from 9419.26953 to 9417.15039, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.326-t9554.12-v9417.15.hdf5\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9554.1182 - val_loss: 9417.1504\n",
      "Epoch 327/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9589.9531\n",
      "Epoch 327: val_loss improved from 9417.15039 to 9403.36816, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.327-t9555.20-v9403.37.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9555.1953 - val_loss: 9403.3682\n",
      "Epoch 328/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9524.4844\n",
      "Epoch 328: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9524.4844 - val_loss: 9440.7998\n",
      "Epoch 329/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9532.7090\n",
      "Epoch 329: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9532.7090 - val_loss: 9440.7744\n",
      "Epoch 330/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9491.7764\n",
      "Epoch 330: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9491.7764 - val_loss: 9493.2891\n",
      "Epoch 331/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9498.7861\n",
      "Epoch 331: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9498.7861 - val_loss: 9507.4873\n",
      "Epoch 332/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9535.9111\n",
      "Epoch 332: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9535.9111 - val_loss: 9488.8740\n",
      "Epoch 333/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9489.2900\n",
      "Epoch 333: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9511.5859 - val_loss: 9569.7832\n",
      "Epoch 334/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9492.1123\n",
      "Epoch 334: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9492.1123 - val_loss: 9562.9053\n",
      "Epoch 335/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9507.9609\n",
      "Epoch 335: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9507.9609 - val_loss: 9416.4121\n",
      "Epoch 336/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9540.4639\n",
      "Epoch 336: val_loss did not improve from 9403.36816\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9516.4180 - val_loss: 9567.0293\n",
      "Epoch 337/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9586.6650\n",
      "Epoch 337: val_loss improved from 9403.36816 to 9327.21680, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.337-t9556.84-v9327.22.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9556.8369 - val_loss: 9327.2168\n",
      "Epoch 338/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9529.7021\n",
      "Epoch 338: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9529.7021 - val_loss: 9464.1270\n",
      "Epoch 339/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9571.0088\n",
      "Epoch 339: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9551.8525 - val_loss: 9560.3262\n",
      "Epoch 340/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9540.5615\n",
      "Epoch 340: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9540.5615 - val_loss: 9438.8125\n",
      "Epoch 341/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9567.3848\n",
      "Epoch 341: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9567.3848 - val_loss: 9393.2949\n",
      "Epoch 342/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9537.0898\n",
      "Epoch 342: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9537.0898 - val_loss: 9356.6143\n",
      "Epoch 343/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9550.4111\n",
      "Epoch 343: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9550.4111 - val_loss: 9453.0049\n",
      "Epoch 344/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9565.3594\n",
      "Epoch 344: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9545.0762 - val_loss: 9516.4434\n",
      "Epoch 345/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9609.4199\n",
      "Epoch 345: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9577.5420 - val_loss: 9351.2236\n",
      "Epoch 346/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9546.4453\n",
      "Epoch 346: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9515.2676 - val_loss: 9392.5361\n",
      "Epoch 347/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9545.4619\n",
      "Epoch 347: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9545.4619 - val_loss: 9467.0381\n",
      "Epoch 348/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9455.9609\n",
      "Epoch 348: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9455.9609 - val_loss: 9506.8184\n",
      "Epoch 349/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9450.7471\n",
      "Epoch 349: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9450.7471 - val_loss: 9604.0664\n",
      "Epoch 350/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9467.4990\n",
      "Epoch 350: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9467.4990 - val_loss: 9503.0771\n",
      "Epoch 351/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9524.6270\n",
      "Epoch 351: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9506.4375 - val_loss: 9760.3467\n",
      "Epoch 352/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9495.9707\n",
      "Epoch 352: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9486.1768 - val_loss: 9717.6191\n",
      "Epoch 353/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9492.5078\n",
      "Epoch 353: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9492.5078 - val_loss: 9341.9805\n",
      "Epoch 354/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9488.2393\n",
      "Epoch 354: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9488.2393 - val_loss: 9405.6201\n",
      "Epoch 355/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9520.0029\n",
      "Epoch 355: val_loss did not improve from 9327.21680\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9496.6260 - val_loss: 9787.2842\n",
      "Epoch 356/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9453.7695\n",
      "Epoch 356: val_loss improved from 9327.21680 to 9326.36719, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.356-t9453.77-v9326.37.hdf5\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9453.7695 - val_loss: 9326.3672\n",
      "Epoch 357/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9512.1221\n",
      "Epoch 357: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9499.6670 - val_loss: 9662.7773\n",
      "Epoch 358/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9509.8359\n",
      "Epoch 358: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9509.8359 - val_loss: 9436.5859\n",
      "Epoch 359/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9476.0566\n",
      "Epoch 359: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9476.0566 - val_loss: 9340.9746\n",
      "Epoch 360/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9478.9717\n",
      "Epoch 360: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9478.9717 - val_loss: 9433.1787\n",
      "Epoch 361/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9520.2119\n",
      "Epoch 361: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9511.2695 - val_loss: 9367.3633\n",
      "Epoch 362/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9484.6650\n",
      "Epoch 362: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9484.6650 - val_loss: 9442.1445\n",
      "Epoch 363/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9495.6299\n",
      "Epoch 363: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9466.7158 - val_loss: 9351.0127\n",
      "Epoch 364/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9468.8828\n",
      "Epoch 364: val_loss did not improve from 9326.36719\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9437.1357 - val_loss: 9341.4199\n",
      "Epoch 365/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9479.3555\n",
      "Epoch 365: val_loss improved from 9326.36719 to 9326.28809, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.365-t9479.36-v9326.29.hdf5\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9479.3555 - val_loss: 9326.2881\n",
      "Epoch 366/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9442.2510\n",
      "Epoch 366: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9442.2510 - val_loss: 9656.1201\n",
      "Epoch 367/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9450.3340\n",
      "Epoch 367: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9441.7402 - val_loss: 9476.5127\n",
      "Epoch 368/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9508.6582\n",
      "Epoch 368: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9508.6582 - val_loss: 9329.2568\n",
      "Epoch 369/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9444.4736\n",
      "Epoch 369: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9444.4736 - val_loss: 9605.2607\n",
      "Epoch 370/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9480.8496\n",
      "Epoch 370: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9452.2129 - val_loss: 9395.2471\n",
      "Epoch 371/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9435.6621\n",
      "Epoch 371: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9435.6621 - val_loss: 9490.0713\n",
      "Epoch 372/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9557.5654\n",
      "Epoch 372: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9539.7402 - val_loss: 9589.7217\n",
      "Epoch 373/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9508.9160\n",
      "Epoch 373: val_loss did not improve from 9326.28809\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9481.8242 - val_loss: 9784.0430\n",
      "Epoch 374/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9527.7842\n",
      "Epoch 374: val_loss improved from 9326.28809 to 9320.93359, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.374-t9527.78-v9320.93.hdf5\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9527.7842 - val_loss: 9320.9336\n",
      "Epoch 375/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9437.6865\n",
      "Epoch 375: val_loss improved from 9320.93359 to 9228.69727, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.375-t9437.69-v9228.70.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9437.6865 - val_loss: 9228.6973\n",
      "Epoch 376/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9490.6748\n",
      "Epoch 376: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9490.6748 - val_loss: 9371.9424\n",
      "Epoch 377/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9500.7529\n",
      "Epoch 377: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9500.7529 - val_loss: 9774.7031\n",
      "Epoch 378/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9468.8008\n",
      "Epoch 378: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9461.9375 - val_loss: 9462.9521\n",
      "Epoch 379/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9489.2959\n",
      "Epoch 379: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9489.2959 - val_loss: 9527.5254\n",
      "Epoch 380/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9446.3682\n",
      "Epoch 380: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9446.3682 - val_loss: 9449.3789\n",
      "Epoch 381/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9457.6338\n",
      "Epoch 381: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9457.6338 - val_loss: 9429.7012\n",
      "Epoch 382/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9458.0625\n",
      "Epoch 382: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9458.0625 - val_loss: 9353.8643\n",
      "Epoch 383/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9448.6826\n",
      "Epoch 383: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9448.6826 - val_loss: 9386.8545\n",
      "Epoch 384/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9462.3252\n",
      "Epoch 384: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9462.3252 - val_loss: 9474.7598\n",
      "Epoch 385/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9432.0625\n",
      "Epoch 385: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9432.0625 - val_loss: 9466.8984\n",
      "Epoch 386/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9418.2920\n",
      "Epoch 386: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9418.2920 - val_loss: 9390.8242\n",
      "Epoch 387/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9463.8936\n",
      "Epoch 387: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9441.9443 - val_loss: 9346.1182\n",
      "Epoch 388/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9412.7246\n",
      "Epoch 388: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9412.7246 - val_loss: 9376.9980\n",
      "Epoch 389/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9403.9043\n",
      "Epoch 389: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9403.9043 - val_loss: 9345.2617\n",
      "Epoch 390/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9410.3115\n",
      "Epoch 390: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9410.3115 - val_loss: 9467.1045\n",
      "Epoch 391/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9444.7197\n",
      "Epoch 391: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9444.7197 - val_loss: 9391.0107\n",
      "Epoch 392/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9444.1533\n",
      "Epoch 392: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9444.1533 - val_loss: 9365.5615\n",
      "Epoch 393/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9434.2246\n",
      "Epoch 393: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9400.3398 - val_loss: 9297.7578\n",
      "Epoch 394/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9456.9141\n",
      "Epoch 394: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9456.9141 - val_loss: 9287.6416\n",
      "Epoch 395/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9425.5273\n",
      "Epoch 395: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9425.5273 - val_loss: 9660.3887\n",
      "Epoch 396/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9390.7197\n",
      "Epoch 396: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9390.7197 - val_loss: 9427.8281\n",
      "Epoch 397/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9380.1934\n",
      "Epoch 397: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9380.1934 - val_loss: 9391.6045\n",
      "Epoch 398/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9407.9678\n",
      "Epoch 398: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9407.9678 - val_loss: 9494.3848\n",
      "Epoch 399/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9404.1670\n",
      "Epoch 399: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9404.1670 - val_loss: 9342.7891\n",
      "Epoch 400/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9405.0234\n",
      "Epoch 400: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9382.2500 - val_loss: 9360.5713\n",
      "Epoch 401/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9394.4775\n",
      "Epoch 401: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 61ms/step - loss: 9394.4775 - val_loss: 9246.1826\n",
      "Epoch 402/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9422.3975\n",
      "Epoch 402: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9393.7637 - val_loss: 9451.4062\n",
      "Epoch 403/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9402.9053\n",
      "Epoch 403: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9402.9053 - val_loss: 9243.4141\n",
      "Epoch 404/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9373.0664\n",
      "Epoch 404: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9373.0664 - val_loss: 9593.0381\n",
      "Epoch 405/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9406.6182\n",
      "Epoch 405: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9394.3604 - val_loss: 9705.6113\n",
      "Epoch 406/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9383.5645\n",
      "Epoch 406: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9383.5645 - val_loss: 9312.3652\n",
      "Epoch 407/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9419.6191\n",
      "Epoch 407: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9413.7002 - val_loss: 9261.0967\n",
      "Epoch 408/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9382.6094\n",
      "Epoch 408: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9368.2529 - val_loss: 9372.8379\n",
      "Epoch 409/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9369.0332\n",
      "Epoch 409: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9369.0332 - val_loss: 9380.3535\n",
      "Epoch 410/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9413.8438\n",
      "Epoch 410: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9379.6182 - val_loss: 9314.2822\n",
      "Epoch 411/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9415.0000\n",
      "Epoch 411: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9392.6270 - val_loss: 9434.1143\n",
      "Epoch 412/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9391.4092\n",
      "Epoch 412: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9391.4092 - val_loss: 9246.4902\n",
      "Epoch 413/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9420.4385\n",
      "Epoch 413: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9420.4385 - val_loss: 9610.4932\n",
      "Epoch 414/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9419.9697\n",
      "Epoch 414: val_loss did not improve from 9228.69727\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9419.9697 - val_loss: 9460.2070\n",
      "Epoch 415/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9439.1816\n",
      "Epoch 415: val_loss improved from 9228.69727 to 9196.66406, saving model to ./trained_models/model-a86fd471-80eNoise-checkpoints/weights.415-t9439.18-v9196.66.hdf5\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9439.1816 - val_loss: 9196.6641\n",
      "Epoch 416/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9398.1865\n",
      "Epoch 416: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9369.7393 - val_loss: 9350.3926\n",
      "Epoch 417/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9443.7529\n",
      "Epoch 417: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9443.7529 - val_loss: 9499.2314\n",
      "Epoch 418/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9463.7432\n",
      "Epoch 418: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 7s 70ms/step - loss: 9436.8887 - val_loss: 9229.9531\n",
      "Epoch 419/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9411.0146\n",
      "Epoch 419: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 7s 72ms/step - loss: 9411.0146 - val_loss: 9331.0635\n",
      "Epoch 420/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9423.3174\n",
      "Epoch 420: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 7s 72ms/step - loss: 9407.1016 - val_loss: 9342.7383\n",
      "Epoch 421/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9370.9268\n",
      "Epoch 421: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 7s 76ms/step - loss: 9370.9268 - val_loss: 9634.7529\n",
      "Epoch 422/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9421.8066\n",
      "Epoch 422: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9421.8066 - val_loss: 9298.2861\n",
      "Epoch 423/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9381.4150\n",
      "Epoch 423: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9381.4150 - val_loss: 9300.6377\n",
      "Epoch 424/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9406.1816\n",
      "Epoch 424: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9405.0156 - val_loss: 9333.6875\n",
      "Epoch 425/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9441.0830\n",
      "Epoch 425: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9441.0830 - val_loss: 9241.1396\n",
      "Epoch 426/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9421.4873\n",
      "Epoch 426: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 7s 73ms/step - loss: 9421.4873 - val_loss: 9367.9795\n",
      "Epoch 427/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9453.4170\n",
      "Epoch 427: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9432.3564 - val_loss: 9453.4658\n",
      "Epoch 428/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9456.4609\n",
      "Epoch 428: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9456.4609 - val_loss: 9287.6426\n",
      "Epoch 429/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9497.7168\n",
      "Epoch 429: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9497.7168 - val_loss: 9367.3340\n",
      "Epoch 430/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9408.1592\n",
      "Epoch 430: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9408.1592 - val_loss: 9374.2383\n",
      "Epoch 431/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9481.1094\n",
      "Epoch 431: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9481.1094 - val_loss: 9687.5801\n",
      "Epoch 432/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9523.8066\n",
      "Epoch 432: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9488.5039 - val_loss: 9675.9453\n",
      "Epoch 433/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9465.0625\n",
      "Epoch 433: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9435.2500 - val_loss: 9777.0742\n",
      "Epoch 434/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9458.3125\n",
      "Epoch 434: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9442.0566 - val_loss: 9540.8965\n",
      "Epoch 435/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9454.9961\n",
      "Epoch 435: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9454.9961 - val_loss: 9432.8203\n",
      "Epoch 436/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9480.5020\n",
      "Epoch 436: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9480.5020 - val_loss: 9370.0479\n",
      "Epoch 437/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9496.2471\n",
      "Epoch 437: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9493.1904 - val_loss: 9317.0205\n",
      "Epoch 438/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9468.1768\n",
      "Epoch 438: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9437.8682 - val_loss: 9213.8926\n",
      "Epoch 439/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9400.6260\n",
      "Epoch 439: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9366.2402 - val_loss: 9326.5791\n",
      "Epoch 440/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9400.6924\n",
      "Epoch 440: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9400.6924 - val_loss: 9208.5264\n",
      "Epoch 441/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9413.4111\n",
      "Epoch 441: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9413.4111 - val_loss: 9284.5264\n",
      "Epoch 442/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9407.3721\n",
      "Epoch 442: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9407.3721 - val_loss: 9722.1055\n",
      "Epoch 443/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9480.3535\n",
      "Epoch 443: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9480.3535 - val_loss: 9307.2598\n",
      "Epoch 444/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9410.9502\n",
      "Epoch 444: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9410.9502 - val_loss: 9401.6982\n",
      "Epoch 445/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9435.4980\n",
      "Epoch 445: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9435.4980 - val_loss: 9388.2725\n",
      "Epoch 446/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9468.1465\n",
      "Epoch 446: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9456.4609 - val_loss: 9202.3809\n",
      "Epoch 447/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9484.4834\n",
      "Epoch 447: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9452.9473 - val_loss: 9267.4746\n",
      "Epoch 448/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9457.2695\n",
      "Epoch 448: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9434.1660 - val_loss: 9749.4922\n",
      "Epoch 449/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9430.8965\n",
      "Epoch 449: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9430.8965 - val_loss: 9273.1963\n",
      "Epoch 450/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9446.6250\n",
      "Epoch 450: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9446.6250 - val_loss: 9317.9062\n",
      "Epoch 451/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9410.6240\n",
      "Epoch 451: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9395.5264 - val_loss: 9273.3877\n",
      "Epoch 452/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9427.6650\n",
      "Epoch 452: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 65ms/step - loss: 9413.4287 - val_loss: 9374.8291\n",
      "Epoch 453/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9389.5234\n",
      "Epoch 453: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9388.3604 - val_loss: 9868.2695\n",
      "Epoch 454/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9471.8047\n",
      "Epoch 454: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9471.8047 - val_loss: 9494.3252\n",
      "Epoch 455/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9407.3398\n",
      "Epoch 455: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 63ms/step - loss: 9405.3174 - val_loss: 9698.8262\n",
      "Epoch 456/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9431.3652\n",
      "Epoch 456: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 68ms/step - loss: 9431.3652 - val_loss: 9646.8643\n",
      "Epoch 457/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9408.5254\n",
      "Epoch 457: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9408.5254 - val_loss: 9227.3643\n",
      "Epoch 458/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9469.0986\n",
      "Epoch 458: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9469.0986 - val_loss: 9459.1396\n",
      "Epoch 459/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9459.6650\n",
      "Epoch 459: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 67ms/step - loss: 9472.8857 - val_loss: 9582.8643\n",
      "Epoch 460/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9428.2930\n",
      "Epoch 460: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9428.2930 - val_loss: 9549.3545\n",
      "Epoch 461/500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 9464.6436\n",
      "Epoch 461: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 62ms/step - loss: 9463.4707 - val_loss: 9414.8164\n",
      "Epoch 462/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9430.8418\n",
      "Epoch 462: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 66ms/step - loss: 9430.8418 - val_loss: 9365.4619\n",
      "Epoch 463/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9426.1699\n",
      "Epoch 463: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 64ms/step - loss: 9426.1699 - val_loss: 9368.6553\n",
      "Epoch 464/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9425.8125\n",
      "Epoch 464: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 70ms/step - loss: 9425.8125 - val_loss: 9289.2979\n",
      "Epoch 465/500\n",
      "93/93 [==============================] - ETA: 0s - loss: 9407.8213\n",
      "Epoch 465: val_loss did not improve from 9196.66406\n",
      "93/93 [==============================] - 6s 69ms/step - loss: 9407.8213 - val_loss: 9458.0811\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        x=training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[es, mcp, csv_logger],\n",
    "        epochs=500,\n",
    "        shuffle=False,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e2706-3e27-4ece-9ebd-be135f6bbcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv]",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
